{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j8eV4AntBVnb",
        "PTiF7Uyg93Tr",
        "7_BI4WVv-TPw",
        "kdvGpP3SfDhh",
        "n_JwLYcTzo2Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFjUU4lBCa0u",
        "outputId": "393b8c84-7e76-4409-b871-aac62d9cfdb0"
      },
      "source": [
        "pip install wfdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.4.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->wfdb) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->wfdb) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->wfdb) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.8.1->wfdb) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXOa3P-wP6hB"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "from keras.models import model_from_json\n",
        "import sys\n",
        "from keras.layers import Convolution1D, MaxPooling1D, Reshape, PReLU, Input, Bidirectional, LSTM\n",
        "from keras.layers import Dense, Dropout,MaxPooling2D, Activation, Flatten, Convolution2D, InputLayer, concatenate\n",
        "from keras.models import Sequential, Model\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import h5py\n",
        "import time\n",
        "import wfdb\n",
        "from scipy.signal import hilbert\n",
        "from tensorflow.keras.utils import plot_model \n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wa9Sr1CBtnT"
      },
      "source": [
        "start_dir = '/content/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pua8sdQT4zf"
      },
      "source": [
        "# **Download Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlF2hou95mfH",
        "outputId": "1d0041ef-1f6a-4adf-9b4f-2e04d7ba3cdd"
      },
      "source": [
        "!wget https://physionet.org/files/challenge-2018/1.0.0/RECORDS?download"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-14 08:00:46--  https://physionet.org/files/challenge-2018/1.0.0/RECORDS?download\n",
            "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
            "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53551 (52K) [text/plain]\n",
            "Saving to: ‘RECORDS?download’\n",
            "\n",
            "RECORDS?download    100%[===================>]  52.30K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-11-14 08:00:47 (756 KB/s) - ‘RECORDS?download’ saved [53551/53551]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGkXOscs0few"
      },
      "source": [
        "f = open(\"/content/RECORDS?download\", 'r')\n",
        "Lines = f.readlines()\n",
        "os.mkdir(\"/content/data/\")\n",
        "os.mkdir(\"/content/data/test/\")\n",
        "os.mkdir(\"/content/data/training\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81PssPyH5zm4"
      },
      "source": [
        "train_counter = 0\n",
        "test_counter = 0\n",
        "for l in Lines:\n",
        "  if str(l).__contains__(\"train\"):\n",
        "    train_counter += 1\n",
        "    if train_counter >= 10:\n",
        "      break\n",
        "    name = l.strip().split('/')[1]\n",
        "    os.mkdir(\"/content/data/training/\" + name + \"/\")\n",
        "    # os.chdir(\"/content/data/test/\" + name + \"/\")\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \"-arousal.mat\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \"-arousal.mat\" , 'wb').write(r.content)\n",
        "\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \".arousal\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \".arousal\" , 'wb').write(r.content)\n",
        "\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \".hea\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \".hea\" , 'wb').write(r.content)\n",
        "\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \".mat\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \".mat\" , 'wb').write(r.content)\n",
        "  else:\n",
        "    test_counter += 1\n",
        "    if test_counter >= 2:\n",
        "        continue\n",
        "    name = l.strip().split('/')[1]\n",
        "    if(os.path.isdir(\"/content/data/test/\" + name + \"/\")):\n",
        "      continue\n",
        "    os.mkdir(\"/content/data/test/\" + name + \"/\")\n",
        "    # os.chdir(\"/content/data/test/\" + name + \"/\")\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \".hea\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \".hea\" , 'wb').write(r.content)\n",
        "\n",
        "    url = 'https://physionet.org/files/challenge-2018/1.0.0/'+ l.strip() + \".mat\"\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('/content/data/' + l.strip() + \".mat\" , 'wb').write(r.content)\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8eV4AntBVnb"
      },
      "source": [
        "# **Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2fPiIZbBT-O",
        "outputId": "3d3c18fd-6d5c-4652-eb1e-259887553e3d"
      },
      "source": [
        "########################################################\n",
        "#Script for preprocessing of raw data into time windows#\n",
        "########################################################\n",
        "\n",
        "\n",
        "#raw data directory\n",
        "fileDir = start_dir + 'data/training/'\n",
        "#parent of directory where to save windowed data\n",
        "resDir = start_dir + 'data/'\n",
        "\n",
        "\n",
        "fileList = os.listdir(fileDir)\n",
        "fileList = [f for f in fileList if f[:2] == 'tr']\n",
        "\n",
        "\n",
        "\n",
        "windowSize = 30\n",
        "downSampling = 4 #downsampling ratio for the signal\n",
        "fs = 200\n",
        "numOfChannels = 13\n",
        "win_size_fs = int(round(windowSize*(fs/float(downSampling)))) #1500\n",
        "overlap = False\n",
        "cumNeg = 0\n",
        "cumPos = 0\n",
        "\n",
        "# -------------------------------------- for all training data\n",
        "for i in range(0,len(fileList)):\n",
        "    start_time = time.time()\n",
        "    print(str(i+1))\n",
        "\n",
        "    currPatient = fileList[i]\n",
        "    currSubFolder =  fileDir + currPatient + '/'\n",
        "    dataFile = currSubFolder + currPatient\n",
        "    val, _ = wfdb.rdsamp(dataFile)\n",
        "    val = np.float16(val)\n",
        "    arousalFile = currSubFolder + currPatient + '-arousal.mat'\n",
        "    with h5py.File(arousalFile, 'r') as f:\n",
        "        arousalLabel = f['data']['arousals']\n",
        "        arousalLabel = arousalLabel[:]\n",
        "         \n",
        "        \n",
        "    val = val[::downSampling,:]\n",
        "\n",
        "\n",
        "    if overlap:\n",
        "        templName = 'overlap_'\n",
        "    else:\n",
        "        templName = 'non_overlap_'\n",
        "        \n",
        "    templName = templName +  str(windowSize)\n",
        "    \n",
        "    userCacheDir = resDir + 'cache/'  \n",
        "    if not os.path.isdir(userCacheDir):\n",
        "        os.mkdir(userCacheDir)\n",
        "    userCacheDir = userCacheDir + templName + '/'\n",
        "    if not os.path.isdir(userCacheDir):\n",
        "        os.mkdir(userCacheDir)\n",
        "    \n",
        "    userCacheDir = userCacheDir + fileList[i] + '/'\n",
        "    if not os.path.isdir(userCacheDir):\n",
        "        os.mkdir(userCacheDir)\n",
        "        \n",
        "    \n",
        "    arousalLabel = arousalLabel[::downSampling]\n",
        "    \n",
        "    length = len(val)\n",
        "    startIdx = 0\n",
        "    finIdx = win_size_fs\n",
        "    if overlap:\n",
        "        deltaWindow = int(np.floor(win_size_fs/2.))\n",
        "    else:\n",
        "        deltaWindow = int(win_size_fs)\n",
        "    \n",
        "    winCount = 1\n",
        "    x_temp = []\n",
        "    y_temp = []\n",
        "    # windowing for each one\n",
        "    while finIdx <= length:\n",
        "        winVal = val[startIdx:finIdx,:]\n",
        "        currLabelVec = np.array(arousalLabel[startIdx:finIdx]);\n",
        "        currLabelVoting = [np.sum(currLabelVec == -1),np.sum(currLabelVec == 0),\n",
        "                           np.sum(currLabelVec == 1)]\n",
        "        idx = np.argmax(currLabelVoting);\n",
        "        idx = idx - 1;\n",
        "        #consider max number of 0 -1 and 1 in each window(vote)\n",
        "        y_temp.append(idx)\n",
        "        x_temp.append(winVal)\n",
        "        winCount = winCount + 1\n",
        "        startIdx = startIdx + deltaWindow\n",
        "        finIdx = finIdx + deltaWindow\n",
        "    x_temp = np.array(x_temp,dtype = np.float16 )\n",
        "    y_aux = []\n",
        "    valid_count = 0;\n",
        "    # consider number of arousal marked windows\n",
        "    pos = np.sum(np.asarray(y_temp) == 1.)\n",
        "    neg = np.sum(np.asarray(y_temp) == 0.)\n",
        "    \n",
        "    for j in range(np.size(x_temp,0)):\n",
        "        if y_temp[j] >= 0: # do not consider -1 marked\n",
        "            pickle.dump(x_temp[j],open(userCacheDir + 'x_' + str(valid_count) + '.p','wb+'))\n",
        "            \n",
        "            if j > 0:\n",
        "                prevIdx = j - 1\n",
        "            else:\n",
        "                prevIdx = j\n",
        "            if y_temp[prevIdx] == -1:\n",
        "                pickle.dump(x_temp[prevIdx],open(userCacheDir + 'x_prev_' + str(valid_count) + '.p','wb+'))\n",
        "                \n",
        "            if j < (len(y_temp) - 1):\n",
        "                nextIdx = j + 1\n",
        "            else:\n",
        "                nextIdx = j\n",
        "            if y_temp[nextIdx] == -1:\n",
        "                pickle.dump(x_temp[nextIdx],open(userCacheDir + 'x_next_' + str(valid_count) + '.p','wb+'))\n",
        "            \n",
        "            valid_count = valid_count + 1\n",
        "            y_aux.append(y_temp[j])\n",
        "  \n",
        "    pickle.dump(np.asarray(y_aux,dtype = np.int16),open(userCacheDir + 'y.p','wb+'))\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "--- 2.113645076751709 seconds ---\n",
            "2\n",
            "--- 1.5264689922332764 seconds ---\n",
            "3\n",
            "--- 2.2337214946746826 seconds ---\n",
            "4\n",
            "--- 1.6484529972076416 seconds ---\n",
            "5\n",
            "--- 1.8462467193603516 seconds ---\n",
            "6\n",
            "--- 1.8016185760498047 seconds ---\n",
            "7\n",
            "--- 1.945340633392334 seconds ---\n",
            "8\n",
            "--- 1.6989026069641113 seconds ---\n",
            "9\n",
            "--- 1.7636492252349854 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTiF7Uyg93Tr"
      },
      "source": [
        "# **Main part**\n",
        "1.   Define the parameter \n",
        "2.   Define Model's parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8579kG66jPL"
      },
      "source": [
        "############################################################\n",
        "###CONFIGURATION STUFF, depend on specifc of machine   #####\n",
        "############################################################\n",
        "numOfCPU = 2\n",
        "workers = 2\n",
        "dataDir = start_dir + 'data/'\n",
        "#setting GPU device to use.\n",
        "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
        "\n",
        "#setting maximum number of cores in tensorflow backend\n",
        "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=numOfCPU, inter_op_parallelism_threads=numOfCPU, \\\n",
        "                        allow_soft_placement=True, device_count = {'CPU': numOfCPU})\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "K.set_session(session)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FxW66Cs7c0Q"
      },
      "source": [
        "#####################################\n",
        "###SETTING UP SOME PARAMETERS...#####\n",
        "#####################################\n",
        "training2do = True\n",
        "validation2do = True\n",
        "testing2do = True\n",
        "totalNumOfUser = 7\n",
        "siameseNet = 2\n",
        "channels2Use = [[3],[6],[8],[10],[11],[12]]\n",
        "convModelDims = [1,1,1,1,1,1]\n",
        "typeOfModels = ['new_cnn2','new_cnn2','new_cnn2','new_cnn2','mlp','new_cnn2']\n",
        "mergingMLP = 'a_bit_deep_wider'\n",
        "# modelID = '3_6_8_10_11_12'\n",
        "useBatchNorm = False\n",
        "prelu = True\n",
        "winSize = 30\n",
        "modelID = \"1\"\n",
        "data_path = start_dir + 'data/cache/non_overlap_' + str(winSize) + '/'\n",
        "# data_path = dataDir + 'cache/non_overlap_' + str(winSize) + '/'\n",
        "test_data_dir = start_dir + 'data/test/'\n",
        "batch_size = 32\n",
        "useGenerator = False\n",
        "n_ep = 30\n",
        "samplesPerWin = 1400\n",
        "valNum = 1\n",
        "testNum = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vniU2XxF7Zg8",
        "outputId": "9d4420d9-d1e4-4160-fb19-7355d1bb2ad9"
      },
      "source": [
        "######################################################\n",
        "###DEFINING TRAINING, VALIDATION AND TEST SETS...#####\n",
        "######################################################\n",
        "def get_num_of_samples(data_dir,userIDs):\n",
        "    n_samples = 0\n",
        "    n_pos = 0\n",
        "    n_neg = 0\n",
        "    for currUser in userIDs:\n",
        "        print(data_dir + currUser + '/y.p')\n",
        "        y = pickle.load(open(data_dir + currUser + '/y.p', 'rb'))\n",
        "        n_samples = n_samples + len(y)\n",
        "        n_pos = n_pos + np.sum(   np.asarray(y) == 1   )\n",
        "        n_neg = n_neg + np.sum(   np.asarray(y) == 0   )\n",
        "    \n",
        "    return n_samples, n_pos, n_neg\n",
        "\n",
        "users = os.listdir(data_path)\n",
        "users = [u for u in users if u[:2] == 'tr']\n",
        "users = users[:totalNumOfUser]\n",
        "users.sort()\n",
        "\n",
        "if testing2do:\n",
        "    test_users = os.listdir(test_data_dir)\n",
        "    test_users = [u for u in test_users if u[:2] == 'te']\n",
        "    test_users.sort()\n",
        "\n",
        "userIDs = users[(valNum+testNum):]\n",
        "testUserIDs = users[:testNum]\n",
        "valUserIDs = users[testNum:(testNum+valNum)]\n",
        "\n",
        "n_samples, n_pos, n_neg = get_num_of_samples(data_path,userIDs)\n",
        "\n",
        "print('num of samples: ' + str(n_samples))\n",
        "print('Positive to Negative Ratio: ' + str( float(n_pos)/float(n_neg) ))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/cache/non_overlap_30/tr03-0052/y.p\n",
            "/content/data/cache/non_overlap_30/tr03-0078/y.p\n",
            "/content/data/cache/non_overlap_30/tr03-0079/y.p\n",
            "/content/data/cache/non_overlap_30/tr03-0086/y.p\n",
            "/content/data/cache/non_overlap_30/tr03-0087/y.p\n",
            "num of samples: 3280\n",
            "Positive to Negative Ratio: 0.028858218318695106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh5s_72j92hp"
      },
      "source": [
        "######################################################\n",
        "###DEFINING Layes Parameter                      #####\n",
        "######################################################\n",
        "newConvFilters = [8   ,8    ,8   ,16  ,32  ,32]\n",
        "newConvKernels = [16   ,16    ,16   ,32  ,32   ,64]\n",
        "newConvPool = [False,True,False,True,False,True]\n",
        "# channels2Use = [[3],[6],[7],[9],[11],[12]]\n",
        "convFilters = []\n",
        "convKernels = []\n",
        "convPool = []\n",
        "convDO = []\n",
        "\n",
        "denseUnits = []\n",
        "denseDO = []\n",
        "rnnUnits = []\n",
        "rnnDO = []\n",
        "mergingDenseUnits = [1024,512,256]\n",
        "mergingDO = [0.33,0.33,0.33]\n",
        "input_dims = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWx2P5CH-758"
      },
      "source": [
        "for i in range(0,4):\n",
        "  convFilters.append(newConvFilters)\n",
        "  convKernels.append(newConvKernels)\n",
        "  convPool.append(newConvPool)\n",
        "  convDO.append(0.33)\n",
        "  denseUnits.append([256,128,128])\n",
        "  denseDO.append([0.33])\n",
        "\n",
        "convFilters.append([])\n",
        "convKernels.append([])\n",
        "convPool.append([])\n",
        "convDO.append(0.33)\n",
        "denseUnits.append([512,256,64,64])\n",
        "denseDO.append([0.33])\n",
        "\n",
        "convFilters.append(newConvFilters)\n",
        "convKernels.append(newConvKernels)\n",
        "convPool.append(newConvPool)\n",
        "convDO.append(0.33)\n",
        "denseUnits.append([256,128,128])\n",
        "denseDO.append([0.33])\n",
        "\n",
        "for i in range(0,6):\n",
        "    rnnUnits.append([])\n",
        "    rnnDO.append(0.2)\n",
        "    input_dims.append((samplesPerWin, 1, len(channels2Use[i]) ))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_BI4WVv-TPw"
      },
      "source": [
        "# **Make Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRLvEoeZcEaU"
      },
      "source": [
        "def shallow_model(input_dim,useBatchNorm,convFilters,convKernels,convPool,convDO,denseUnits,denseDO,rnnUnits,rnnDO,convModelDim):\n",
        "    \n",
        "    reg = l2(0.00001)\n",
        "    model=Sequential()\n",
        "    model.add(InputLayer(input_shape=( input_dim[0] , input_dim[1], input_dim[2]  ) ))\n",
        "    if convModelDim == 1:\n",
        "        model.add(Reshape((input_dim[0], input_dim[2]  ) ))\n",
        "\n",
        "      \n",
        "    #Convolutional layers block:\n",
        "    for i in range(len(convFilters)):\n",
        "        if convModelDim == 1:\n",
        "            model.add(Convolution1D(filters=convFilters[i], kernel_size=convKernels[i],\n",
        "                                            padding='same',kernel_regularizer=reg,bias_regularizer=reg))\n",
        "        else:\n",
        "            model.add(Convolution2D(filters=convFilters[i], kernel_size=convKernels[i],\n",
        "                                            padding='same',kernel_regularizer=reg,bias_regularizer=reg))\n",
        "        if convPool[i]:\n",
        "            if convModelDim == 1:\n",
        "                model.add(MaxPooling1D())\n",
        "            else:\n",
        "                model.add(MaxPooling2D())\n",
        "        if useBatchNorm:\n",
        "            model.add(BatchNormalization(scale=False))\n",
        "            model.add(Activation('relu'))\n",
        "        else:\n",
        "            model.add(PReLU(shared_axes=[1]))\n",
        "        model.add(Dropout(convDO))\n",
        "\n",
        "    ret = True\n",
        "    for i in range(len(rnnUnits)):\n",
        "        if i == (len(rnnUnits) -1):\n",
        "            ret = False            \n",
        "        model.add(Bidirectional(LSTM(units=rnnUnits[i],return_sequences=ret,\n",
        "                                             dropout=rnnDO,kernel_regularizer=reg, recurrent_regularizer=reg  )))\n",
        "\n",
        "    \n",
        "    #flattening up filters\n",
        "    if ret:\n",
        "        model.add(Flatten())\n",
        "\n",
        "    #\n",
        "    for i in range(len(denseUnits)):\n",
        "        model.add(Dense(units=denseUnits[i],kernel_regularizer=reg,bias_regularizer=reg))\n",
        "        if useBatchNorm:\n",
        "            model.add(BatchNormalization(scale=False))\n",
        "            model.add(Activation('relu'))\n",
        "        else:\n",
        "            model.add(PReLU(shared_axes=[1]))\n",
        "        model.add(Dropout(denseDO[min(i,len(denseDO) - 1)]))\n",
        "  \n",
        "    return model \n",
        "    \n",
        "def merge_conv_models(useBatchNorm,input_dims,convModelDims,siameseNet,prelu,convFilters,convKernels,convPool,convDO,\n",
        "                      denseUnits,denseDO,rnnUnits,rnnDO,mergingDenseUnits,mergingDO):\n",
        "    model_inputs = []\n",
        "    model_fun = []\n",
        "    curr_models = []\n",
        "    for i in range(len(input_dims)):\n",
        "        curr_models.append(shallow_model(input_dims[i],useBatchNorm,convFilters[i],convKernels[i],convPool[i],\n",
        "                                             convDO[i],denseUnits[i],denseDO[i],rnnUnits[i],rnnDO[i],convModelDims[i]))\n",
        "        model_inputs.append(Input(input_dims[i]))\n",
        "        model_fun.append(curr_models[i](model_inputs[i]))\n",
        "        \n",
        "    for _ in range(siameseNet):        \n",
        "        for i in range(len(input_dims)):\n",
        "            model_inputs.append(Input(input_dims[i]))\n",
        "            model_fun.append(curr_models[i](model_inputs[-1]))\n",
        "    if len(model_fun) > 1:\n",
        "        merged = concatenate(model_fun)\n",
        "        \n",
        "    else:\n",
        "        merged = model_fun[0]\n",
        "        \n",
        "        \n",
        "    for i in range(len(mergingDenseUnits)):\n",
        "        merged = Dense(units=mergingDenseUnits[i],kernel_regularizer=l2(0.00001),bias_regularizer=l2(0.00001))(merged)\n",
        "        merged = PReLU(shared_axes=[1])(merged)\n",
        "        merged = Dropout(mergingDO[i])(merged)\n",
        "    pred_layer = Dense(units=2,kernel_regularizer=l2(0.00001),bias_regularizer=l2(0.00001))(merged)\n",
        "    pred_layer = Activation('softmax')(pred_layer)\n",
        "    model = Model(model_inputs,pred_layer)\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc9P5Gzl-6Wq",
        "outputId": "85ac51ef-c736-4f17-b00d-acba4d1ef73d"
      },
      "source": [
        "model = merge_conv_models(useBatchNorm,input_dims,convModelDims,siameseNet,prelu,\n",
        "                          convFilters,convKernels,convPool,convDO,denseUnits,denseDO,rnnUnits,\n",
        "                          rnnDO,\n",
        "                          mergingDenseUnits,\n",
        "                          mergingDO)    \n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "optimizer = Adam(lr=0.00005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_17 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_19 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_21 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 1400, 1, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 128)          1571667     ['input_2[0][0]',                \n",
            "                                                                  'input_13[0][0]',               \n",
            "                                                                  'input_19[0][0]']               \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 128)          1571667     ['input_4[0][0]',                \n",
            "                                                                  'input_14[0][0]',               \n",
            "                                                                  'input_20[0][0]']               \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 128)          1571667     ['input_6[0][0]',                \n",
            "                                                                  'input_15[0][0]',               \n",
            "                                                                  'input_21[0][0]']               \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 128)          1571667     ['input_8[0][0]',                \n",
            "                                                                  'input_16[0][0]',               \n",
            "                                                                  'input_22[0][0]']               \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 64)           869252      ['input_10[0][0]',               \n",
            "                                                                  'input_17[0][0]',               \n",
            "                                                                  'input_23[0][0]']               \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)      (None, 128)          1571667     ['input_12[0][0]',               \n",
            "                                                                  'input_18[0][0]',               \n",
            "                                                                  'input_24[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2112)         0           ['sequential[0][0]',             \n",
            "                                                                  'sequential_1[0][0]',           \n",
            "                                                                  'sequential_2[0][0]',           \n",
            "                                                                  'sequential_3[0][0]',           \n",
            "                                                                  'sequential_4[0][0]',           \n",
            "                                                                  'sequential_5[0][0]',           \n",
            "                                                                  'sequential[1][0]',             \n",
            "                                                                  'sequential_1[1][0]',           \n",
            "                                                                  'sequential_2[1][0]',           \n",
            "                                                                  'sequential_3[1][0]',           \n",
            "                                                                  'sequential_4[1][0]',           \n",
            "                                                                  'sequential_5[1][0]',           \n",
            "                                                                  'sequential[2][0]',             \n",
            "                                                                  'sequential_1[2][0]',           \n",
            "                                                                  'sequential_2[2][0]',           \n",
            "                                                                  'sequential_3[2][0]',           \n",
            "                                                                  'sequential_4[2][0]',           \n",
            "                                                                  'sequential_5[2][0]']           \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 1024)         2163712     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " p_re_lu_49 (PReLU)             (None, 1024)         1           ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 1024)         0           ['p_re_lu_49[0][0]']             \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 512)          524800      ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            " p_re_lu_50 (PReLU)             (None, 512)          1           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 512)          0           ['p_re_lu_50[0][0]']             \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 256)          131328      ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " p_re_lu_51 (PReLU)             (None, 256)          1           ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 256)          0           ['p_re_lu_51[0][0]']             \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 2)            514         ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 2)            0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,547,944\n",
            "Trainable params: 11,547,944\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdvGpP3SfDhh"
      },
      "source": [
        "# **Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuN3MufdfA_A"
      },
      "source": [
        "def generator(data_path,batch_size,user_ids,chs,n_sub_sampled,convModelDims,siameseNet, y_alls_pos, y_alls_neg,winSize):\n",
        "    '''\n",
        "    This is the actual data generator. Super inefficient as it is...\n",
        "    '''\n",
        "    \n",
        "    flattened_channels = [item for sublist in chs for item in sublist]\n",
        "    while True:\n",
        "        x_s = np.zeros((batch_size,n_sub_sampled,len(flattened_channels)),dtype = np.float32)\n",
        "        if siameseNet >= 1:\n",
        "            x_s_prev = np.zeros((batch_size,n_sub_sampled,len(flattened_channels)),dtype = np.float32)\n",
        "            if siameseNet >= 2:\n",
        "                x_s_next = np.zeros((batch_size,n_sub_sampled,len(flattened_channels)),dtype = np.float32)\n",
        "            else:\n",
        "                x_s_next = []\n",
        "        else:\n",
        "            x_s_prev = []\n",
        "            x_s_next = []\n",
        "        y_s = np.zeros((batch_size),dtype = np.int16)\n",
        "\n",
        "        #the first half of the batch is taken from positive arousal sample\n",
        "        for i in range(batch_size//2):\n",
        "            temp_vec = []\n",
        "            while len(temp_vec) == 0:\n",
        "                u_idx = np.random.choice(len(user_ids))\n",
        "                id = user_ids[u_idx]\n",
        "                temp_vec = y_alls_pos[u_idx]\n",
        "            rand_idx = np.random.choice(temp_vec)\n",
        "            y_s[i] = np.int16(1)\n",
        "            x_temp = np.asarray(pickle.load(open(data_path + id + '/' + 'x_' + str(rand_idx) + '.p', 'rb')),dtype = np.float32)\n",
        "            x_temp = x_temp[:,flattened_channels]\n",
        "            x_s[i] = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)\n",
        "            if siameseNet >= 1:\n",
        "                prev_file_name = get_prev_file_name(data_path + id +  '/',rand_idx )\n",
        "                x_temp = np.asarray(pickle.load(open(prev_file_name,'rb')),dtype = np.float32)\n",
        "                x_temp = x_temp[:,flattened_channels]\n",
        "                x_s_prev[i]  = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)                    \n",
        "                if siameseNet == 2:\n",
        "                    len_yall = len(y_alls_pos[u_idx]) + len(y_alls_neg[u_idx])\n",
        "                    next_file_name = get_next_file_name(data_path + id +  '/',rand_idx,len_yall)\n",
        "                    x_temp = np.asarray(pickle.load(open(next_file_name,'rb')),dtype = np.float32)\n",
        "                    x_temp = x_temp[:,flattened_channels]\n",
        "                    x_s_next[i] = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)\n",
        "        \n",
        "        #the second half of the batch is taken from negative arousal sample          \n",
        "        for i in range(batch_size//2,batch_size):\n",
        "            u_idx = np.random.choice(len(user_ids))\n",
        "            id = user_ids[u_idx]\n",
        "            rand_idx = np.random.choice(y_alls_neg[u_idx])\n",
        "            y_s[i] = np.int16(0)\n",
        "            x_temp = np.asarray(pickle.load(open(data_path + id + '/' + 'x_' + str(rand_idx) + '.p', 'rb')),dtype = np.float32)\n",
        "            x_temp = x_temp[:,flattened_channels]\n",
        "            x_s[i] = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)\n",
        "            if siameseNet >= 1:\n",
        "                prev_file_name = get_prev_file_name(data_path + id +  '/',rand_idx )\n",
        "                x_temp = np.asarray(pickle.load(open(prev_file_name,'rb')),dtype = np.float32)\n",
        "                x_temp = x_temp[:,flattened_channels]\n",
        "                x_s_prev[i]  = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)                    \n",
        "                if siameseNet == 2:\n",
        "                    len_yall = len(y_alls_pos[u_idx]) + len(y_alls_neg[u_idx])\n",
        "                    next_file_name = get_next_file_name(data_path + id +  '/',rand_idx,len_yall)\n",
        "                    x_temp = np.asarray(pickle.load(open(next_file_name,'rb')),dtype = np.float32)\n",
        "                    x_temp = x_temp[:,flattened_channels]\n",
        "                    x_s_next[i]  = pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize)\n",
        "   \n",
        "                    \n",
        "        x_s = rearrange_channels(x_s,convModelDims,chs)           \n",
        "        \n",
        "        if siameseNet >= 1:\n",
        "            x_s_prev = rearrange_channels(x_s_prev,convModelDims,chs)   \n",
        "            if siameseNet == 2:\n",
        "                x_s_next = rearrange_channels(x_s_next,convModelDims,chs) \n",
        "        \n",
        "        yield x_s + x_s_prev + x_s_next, to_categorical(y_s,num_classes = 2)\n",
        "    \n",
        "   \n",
        "def comp_inst_phase(x_temp):\n",
        "    fs = 50.0    #carefull! sampling frequency is hard coded here...\n",
        "    analytic_signal = hilbert(x_temp)\n",
        "    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
        "    instantaneous_frequency = (np.diff(instantaneous_phase) /(2.0*np.pi) * fs)\n",
        "    instantaneous_frequency = np.append(instantaneous_frequency,instantaneous_frequency[-1])\n",
        "    return instantaneous_frequency\n",
        "    \n",
        "def per_sample_x_axis_sub_sample(x_train,x_axis_size):\n",
        "    '''\n",
        "    cropping time signlas for data augmentation\n",
        "    '''\n",
        "    diff = x_train.shape[0] - x_axis_size \n",
        "    intIdx = np.random.randint(diff)\n",
        "    finIdx = diff - intIdx\n",
        "    return x_train[intIdx:(x_train.shape[0]-finIdx),:]\n",
        "\n",
        "def pre_processing(x_temp,flattened_channels,n_sub_sampled,winSize, normFlag = True):\n",
        "    #pre-processing of signals. This include standardisation, and frequency computation for EEG\n",
        "    data_means = [-1.58837489e-04, -1.11909714e-04,  1.85214086e-05,  3.43906861e-06,\n",
        "                      -4.20280479e-04, -4.93160169e-06,  1.79198553e-05,  7.16557753e-05,\n",
        "                      -4.05202774e-03, -1.71058490e-01, -1.55950109e-02,  9.34850143e+01,\n",
        "                      -1.74383175e-06]\n",
        "    data_std = [3.82136545e+01, 4.03138769e+01, 3.92392071e+01, 4.47130111e+01,\n",
        "                    3.96919219e+01, 4.08547734e+01, 3.77899386e+01, 5.63194308e+01,\n",
        "                    6.32337194e+02, 3.99307484e+02, 5.43303006e+02, 8.40357161e+00,\n",
        "                    2.09212648e-01]\n",
        "    if normFlag:\n",
        "        for j in range(x_temp.shape[1]):\n",
        "            x_temp[:,j] = (x_temp[:,j] - data_means[flattened_channels[j]])/data_std[flattened_channels[j]]\n",
        "    for j in range(len(flattened_channels)):\n",
        "        if (flattened_channels[j]>=0) and (flattened_channels[j]<=5): # if it is EEG channel\n",
        "            x_temp[:,j] = comp_inst_phase(x_temp[:,j])\n",
        "    if n_sub_sampled < x_temp.shape[0]:\n",
        "        x_s_i = per_sample_x_axis_sub_sample(x_temp,n_sub_sampled)\n",
        "    else:\n",
        "        x_s_i = x_temp\n",
        "    return x_s_i\n",
        "  \n",
        "def get_new_indexing(channels2Use):\n",
        "    channels2Use_flat = [item for sublist in channels2Use for item in sublist]\n",
        "    new_indexing = []\n",
        "    for ch_list in channels2Use: \n",
        "        curr_new_indexing = []\n",
        "        for ch in ch_list:\n",
        "            curr_new_indexing.append([i for i, c in enumerate(channels2Use_flat) if c == ch][0])\n",
        "        new_indexing.append(curr_new_indexing)\n",
        "    \n",
        "    return new_indexing\n",
        "\n",
        "def rearrange_channels(x_s,convModelDims,chs):\n",
        "    '''\n",
        "    re-arranging channels in a way that is compatible with keras input for training\n",
        "    '''\n",
        "    x_s = x_s[:,:,:,np.newaxis]\n",
        "    x_s = [x_s[:,:,c_s,:] if convModelDims[j] == 2 else np.transpose(x_s[:,:,c_s,:],[0,1,3,2])\n",
        "            for j, c_s in enumerate(get_new_indexing(chs))]\n",
        "    return x_s\n",
        "\n",
        "def get_prev_file_name(preName,rand_idx):\n",
        "    '''\n",
        "    utils to get name of previous data sample. This is an ugly function really, it is necessary because of bad choices made at the\n",
        "    beginning of the coding. It should be removed, and things should get cleaned up a bit\n",
        "    '''\n",
        "    if os.path.isfile(preName + 'x_prev_' + str(rand_idx) + '.p'):\n",
        "        prev_file_name = preName + 'x_prev_' + str(rand_idx) + '.p'\n",
        "    else:\n",
        "        prevIdx = max(rand_idx - 1,0)\n",
        "        prev_file_name = preName + 'x_' + str(prevIdx) + '.p'\n",
        "    return prev_file_name\n",
        "\n",
        "def get_next_file_name(preName,rand_idx,len_y_s):\n",
        "    '''\n",
        "    same as above but for the successive input sample\n",
        "    '''\n",
        "    if os.path.isfile(preName + 'x_next_' + str(rand_idx) + '.p'):\n",
        "        next_file_name = preName + 'x_next_' + str(rand_idx) + '.p'\n",
        "    else:\n",
        "        nextIdx = min(rand_idx + 1,len_y_s - 1)\n",
        "        next_file_name = preName + 'x_' + str(nextIdx) + '.p'\n",
        "    return next_file_name\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_JwLYcTzo2Z"
      },
      "source": [
        "# **Scoring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmpshfyzm4E"
      },
      "source": [
        "import argparse\n",
        "\n",
        "class Challenge2018Score:\n",
        "    \"\"\"Class used to compute scores for the 2018 PhysioNet/CinC Challenge.\n",
        "    A Challenge2018Score object aggregates the outputs of a proposed\n",
        "    classification algorithm, and calculates the area under the\n",
        "    precision-recall curve, as well as the area under the receiver\n",
        "    operating characteristic curve.\n",
        "    After creating an instance of this class, call score_record() for\n",
        "    each record being tested.  To calculate scores for a particular\n",
        "    record, call record_auprc() and record_auroc().  After scoring all\n",
        "    records, call gross_auprc() and gross_auroc() to obtain the scores\n",
        "    for the database as a whole.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_digits=None):\n",
        "        \"\"\"Initialize a new scoring buffer.\n",
        "        If 'input_digits' is given, it is the number of decimal digits\n",
        "        of precision used in input probability values.\n",
        "        \"\"\"\n",
        "        if input_digits is None:\n",
        "            input_digits = 3\n",
        "        self._scale = 10**input_digits\n",
        "        self._pos_values = np.zeros(self._scale + 1, dtype=np.int64)\n",
        "        self._neg_values = np.zeros(self._scale + 1, dtype=np.int64)\n",
        "        self._record_auc = {}\n",
        "\n",
        "    def score_record(self, truth, predictions, record_name=None):\n",
        "        \"\"\"Add results for a given record to the buffer.\n",
        "        'truth' is a vector of arousal values: zero for non-arousal\n",
        "        regions, positive for target arousal regions, and negative for\n",
        "        unscored regions.\n",
        "        'predictions' is a vector of probabilities produced by the\n",
        "        classification algorithm being tested.  This vector must be\n",
        "        the same length as 'truth', and each value must be between 0\n",
        "        and 1.\n",
        "        If 'record_name' is specified, it can be used to obtain\n",
        "        per-record scores afterwards, by calling record_auroc() and\n",
        "        record_auprc().\n",
        "        \"\"\"\n",
        "        # Check if length is correct\n",
        "        if len(predictions) != len(truth):\n",
        "            raise ValueError(\"length of 'predictions' does not match 'truth'\")\n",
        "\n",
        "        # Compute the histogram of all input probabilities\n",
        "        b = self._scale + 1\n",
        "        r = (-0.5 / self._scale, 1.0 + 0.5 / self._scale)\n",
        "        all_values = np.histogram(predictions, bins=b, range=r)[0]\n",
        "\n",
        "        # Check if input contains any out-of-bounds or NaN values\n",
        "        # (which are ignored by numpy.histogram)\n",
        "        if np.sum(all_values) != len(predictions):\n",
        "            raise ValueError(\"invalid values in 'predictions'\")\n",
        "\n",
        "        # Compute the histogram of probabilities within arousal regions\n",
        "        pred_pos = []\n",
        "        pred_ign = []\n",
        "        i = 0\n",
        "        for x in truth:\n",
        "          if int(x) > int(0):\n",
        "            pred_pos.append(predictions[i])\n",
        "          if int(x) < 0:\n",
        "            pred_ign.append(predictions[i])   \n",
        "          i += 1      \n",
        "        pos_values = np.histogram(pred_pos, bins=b, range=r)[0]\n",
        "\n",
        "        # Compute the histogram of probabilities within unscored regions\n",
        "        ign_values = np.histogram(pred_ign, bins=b, range=r)[0]\n",
        "\n",
        "        # Compute the histogram of probabilities in non-arousal regions,\n",
        "        # given the above\n",
        "        neg_values = all_values - pos_values - ign_values\n",
        "\n",
        "        self._pos_values += pos_values\n",
        "        self._neg_values += neg_values\n",
        "\n",
        "        if record_name is not None:\n",
        "            self._record_auc[record_name] = self._auc(pos_values, neg_values)\n",
        "\n",
        "    def _auc(self, pos_values, neg_values):\n",
        "        # Calculate areas under the ROC and PR curves by iterating\n",
        "        # over the possible threshold values.\n",
        "\n",
        "        # At the minimum threshold value, all samples are classified as\n",
        "        # positive, and thus TPR = 1 and TNR = 0.\n",
        "        tp = np.sum(pos_values)\n",
        "        fp = np.sum(neg_values)\n",
        "        tn = fn = 0\n",
        "        tpr = 1\n",
        "        tnr = 0\n",
        "        if tp == 0 or fp == 0:\n",
        "            # If either class is empty, scores are undefined.\n",
        "            return (float('nan'), float('nan'))\n",
        "        ppv = float(tp) / (tp + fp)\n",
        "        auroc = 0\n",
        "        auprc = 0\n",
        "\n",
        "        # As the threshold increases, TP decreases (and FN increases)\n",
        "        # by pos_values[i], while TN increases (and FP decreases) by\n",
        "        # neg_values[i].\n",
        "        for (n_pos, n_neg) in zip(pos_values, neg_values):\n",
        "            tp -= n_pos\n",
        "            fn += n_pos\n",
        "            fp -= n_neg\n",
        "            tn += n_neg\n",
        "            tpr_prev = tpr\n",
        "            tnr_prev = tnr\n",
        "            ppv_prev = ppv\n",
        "            tpr = float(tp) / (tp + fn)\n",
        "            tnr = float(tn) / (tn + fp)\n",
        "            if tp + fp > 0:\n",
        "                ppv = float(tp) / (tp + fp)\n",
        "            else:\n",
        "                ppv = ppv_prev\n",
        "            auroc += (tpr_prev - tpr) * (tnr + tnr_prev) * 0.5\n",
        "            auprc += (tpr_prev - tpr) * ppv_prev\n",
        "        return (auroc, auprc)\n",
        "\n",
        "    def gross_auroc(self):\n",
        "        \"\"\"Compute the area under the ROC curve.\n",
        "        The result will be NaN if none of the records processed so far\n",
        "        contained any target arousals.\n",
        "        \"\"\"\n",
        "        return self._auc(self._pos_values, self._neg_values)[0]\n",
        "\n",
        "    def gross_auprc(self):\n",
        "        \"\"\"Compute the area under the precision-recall curve.\n",
        "        The result will be NaN if none of the records processed so far\n",
        "        contained any target arousals.\n",
        "        \"\"\"\n",
        "        return self._auc(self._pos_values, self._neg_values)[1]\n",
        "\n",
        "    def record_auroc(self, record_name):\n",
        "        \"\"\"Compute the area under the ROC curve for a single record.\n",
        "        The result will be NaN if the record did not contain any\n",
        "        target arousals.\n",
        "        The given record must have previously been processed by\n",
        "        calling score_record().\n",
        "        \"\"\"\n",
        "        return self._record_auc[record_name][0]\n",
        "\n",
        "    def record_auprc(self, record_name):\n",
        "        \"\"\"Compute the area under the PR curve for a single record.\n",
        "        The result will be NaN if the record did not contain any\n",
        "        target arousals.\n",
        "        The given record must have previously been processed by\n",
        "        calling score_record().\n",
        "        \"\"\"\n",
        "        return self._record_auc[record_name][1]\n",
        "\n",
        "\n",
        "################################################################\n",
        "# Command line interface\n",
        "################################################################\n",
        "\n",
        "\n",
        "# p = argparse.ArgumentParser()\n",
        "# p.add_argument('vecfiles', metavar='RECORD.vec', nargs='+',\n",
        "#                 help='vector of probabilities to score')\n",
        "# p.add_argument('-r', '--reference-dir', metavar='DIR', default='training',\n",
        "#                 help='location of reference arousal.mat files')\n",
        "# args = p.parse_args()\n",
        "\n",
        "# print('Record          AUROC     AUPRC')\n",
        "# print('_______________________________')\n",
        "# s = Challenge2018Score()\n",
        "# failed = 0\n",
        "# for vec_file in args.vecfiles:\n",
        "#     record = os.path.basename(vec_file)\n",
        "#     if record.endswith('.vec'):\n",
        "#         record = record[:-4]\n",
        "\n",
        "#     arousal_file = os.path.join(args.reference_dir, record,\n",
        "#                                 record + '-arousal.mat')\n",
        "#     try:\n",
        "#         # Load reference annotations from the arousal.mat file\n",
        "#         with h5py.File(arousal_file, 'r') as af:\n",
        "#             truth = np.ravel(af['data']['arousals'])\n",
        "\n",
        "#         # Load predictions from the vec file\n",
        "#         predictions = np.zeros(len(truth), dtype=np.float32)\n",
        "#         with open(vec_file, 'rb') as vf:\n",
        "#             i = -1\n",
        "#             for (i, v) in enumerate(vf):\n",
        "#                 try:\n",
        "#                     predictions[i] = v\n",
        "#                 except IndexError:\n",
        "#                     break\n",
        "#             if i != len(truth) - 1:\n",
        "#                 print('Warning: wrong number of samples in %s'\n",
        "#                       % vec_file)\n",
        "\n",
        "#         # Compute and print scores for this record\n",
        "#         s.score_record(truth, predictions, record)\n",
        "#         auroc = s.record_auroc(record)\n",
        "#         auprc = s.record_auprc(record)\n",
        "#         print('%-11s  %8.6f  %8.6f' % (record, auroc, auprc))\n",
        "#     except Exception as exc:\n",
        "#         print(exc)\n",
        "#         print('%-11s  %8s  %8s' % (record, 'error', 'error'))\n",
        "#         failed = 1\n",
        "\n",
        "# # Compute and print overall scores\n",
        "# auroc = s.gross_auroc()\n",
        "# auprc = s.gross_auprc()\n",
        "# print('_______________________________')\n",
        "# print('%-11s  %8.6f  %8.6f' % ('Overall', auroc, auprc))\n",
        "# # sys.exit(failed)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvZiz_RneKDj"
      },
      "source": [
        "# **Model Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDPlMTEYeZml"
      },
      "source": [
        "class ModelTest(Callback):\n",
        "    '''\n",
        "    Definition of callback used for computation of AUC on the valdiation set. Either provide Xt and y_dense (that is the numpy arrays of data)\n",
        "    or the path to data directory and a list of user ids.\n",
        "    '''\n",
        "    def __init__(self, test_every_X_epochs, batch_size, rep_idx, valUserIDs,\n",
        "                 dataDir, samplesPerWin, channels2Use, convModelDims, siameseNet, modelID, useGenerator,winSize ):\n",
        "        super(ModelTest, self).__init__()\n",
        "        \n",
        "        self.test_every_X_epochs = test_every_X_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.bestAUPR = 0.\n",
        "        self.AUPRhist = []\n",
        "        self.rep_idx = rep_idx\n",
        "        self.valUserIDs = valUserIDs\n",
        "        self.dataDir = dataDir\n",
        "        self.samplesPerWin = samplesPerWin\n",
        "        self.channels2Use = channels2Use\n",
        "        self.convModelDims = convModelDims\n",
        "        self.siameseNet = siameseNet\n",
        "        self.modelID = modelID\n",
        "        self.y = self.init_y_field()\n",
        "        self.useGenerator = useGenerator\n",
        "        self.winSize = winSize\n",
        "        if not self.useGenerator:\n",
        "            self.cache_Xt()\n",
        "            self.Xt = []\n",
        "            \n",
        "    #initilise labels for the validation set            \n",
        "    def init_y_field(self):\n",
        "        y = []\n",
        "        for currUser in self.valUserIDs:\n",
        "            y.append(np.int16( pickle.load(open(self.dataDir + currUser + '/' + 'y.p', 'rb'))))\n",
        "        return y\n",
        "    \n",
        "    \n",
        "    # Here, I build up the input data matrix for each user, and I cache it to file for quick RAM menagement.\n",
        "    # It's an ugly solution, it would probably much better to just define some proper data generator for the validation test as well...\n",
        "    def cache_Xt(self):\n",
        "\n",
        "        Xt = []\n",
        "        \n",
        "        flattened_channels = [item for sublist in self.channels2Use for item in sublist]\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i,currUser in enumerate(self.valUserIDs):\n",
        "            sys.stdout.write(\"\\r\" + 'pre-processing for validation user #: ' +  str(i))\n",
        "            sys.stdout.flush()\n",
        "            x_s = np.zeros((len(self.y[i]),self.samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "            if self.siameseNet >= 1:\n",
        "                x_s_prev = np.zeros((len(self.y[i]),self.samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "                if self.siameseNet >= 2:\n",
        "                    x_s_next = np.zeros((len(self.y[i]),self.samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "                else:\n",
        "                    x_s_next = []\n",
        "            else:\n",
        "                x_s_prev = []\n",
        "                x_s_next = []\n",
        "            \n",
        "            for j in range(len(self.y[i])):\n",
        "                x_temp = np.asarray(pickle.load(open(self.dataDir + currUser + '/' + 'x_' + str(j) + '.p','rb')),dtype = np.float32)\n",
        "                x_temp = x_temp[:,flattened_channels]\n",
        "                x_s[j] = pre_processing(x_temp,flattened_channels,self.samplesPerWin,self.winSize)\n",
        "                if self.siameseNet >= 1:\n",
        "                    prev_file_name = get_prev_file_name(self.dataDir + currUser +  '/',j )\n",
        "                    x_temp = np.asarray(pickle.load(open(prev_file_name, 'rb')),dtype = np.float32)\n",
        "                    x_temp = x_temp[:,flattened_channels]\n",
        "                    x_s_prev[j]  = pre_processing(x_temp,flattened_channels,self.samplesPerWin,self.winSize)\n",
        "                    if self.siameseNet == 2:\n",
        "                        next_file_name = get_next_file_name(self.dataDir  + currUser +  '/',j,len(self.y[i]))\n",
        "                        x_temp = np.asarray(pickle.load(open(next_file_name, 'rb')),dtype = np.float32)\n",
        "                        x_temp = x_temp[:,flattened_channels]\n",
        "                        x_s_next[j]  = pre_processing(x_temp,flattened_channels,self.samplesPerWin,self.winSize)\n",
        "            x_s = rearrange_channels(x_s,self.convModelDims,self.channels2Use)\n",
        "            if self.siameseNet >= 1:\n",
        "                x_s_prev = rearrange_channels(x_s_prev,self.convModelDims,self.channels2Use)   \n",
        "                if self.siameseNet == 2:\n",
        "                    x_s_next = rearrange_channels(x_s_next,self.convModelDims,self.channels2Use) \n",
        "            Xt.append(x_s + x_s_prev + x_s_next)\n",
        "        print() \n",
        "        print('elapsed time for validation set pre-processing: ' + str(time.time() - start_time))\n",
        "        with open(self.dataDir + 'cached_Xt.p','wb') as f:\n",
        "            pickle.dump(Xt,f)\n",
        "        return \n",
        "    \n",
        "    # Simple method that just loads the model that was pickled in the function above.\n",
        "    def load_Xt(self):\n",
        "        with open(self.dataDir + 'cached_Xt.p', 'rb') as f:\n",
        "            Xt = pickle.load(f)\n",
        "        return Xt\n",
        "    \n",
        "    #method called by keras at the beginning of each epoch. In use it to compute statistics on validation and test set\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch % self.test_every_X_epochs != 0:\n",
        "            return\n",
        "        start_time = time.time()\n",
        "        \n",
        "        \n",
        "        scoreObj = Challenge2018Score() #defining scoring object\n",
        "        self.Xt = self.load_Xt() #loading cached validation set\n",
        "        \n",
        "        #looping on all the validation users\n",
        "        for i, currUser in enumerate(self.valUserIDs):\n",
        "            prediction = self.model.predict(self.Xt[i], batch_size=self.batch_size)\n",
        "            if len(self.y[i]) > len(prediction):\n",
        "              self.y[i] = self.y[i][:len(prediction)]\n",
        "            prediction = prediction[:len(self.y[i]),1]\n",
        "            scoreObj.score_record(self.y[i], prediction, currUser)\n",
        "            \n",
        "        self.Xt = []    #emptying the Xt field\n",
        "        auroc = scoreObj.gross_auroc()\n",
        "        auprc = scoreObj.gross_auprc()\n",
        "        self.AUPRhist.append(auprc)\n",
        "        \n",
        "        if auprc > self.bestAUPR:\n",
        "            self.bestAUPR = auprc        \n",
        "            model_json = self.model.to_json()\n",
        "            fileName = self.dataDir + self.modelID +  \"_model\"\n",
        "            with open(fileName + \".json\", \"w+\") as json_file:\n",
        "                json_file.write(model_json)\n",
        "            self.model.save_weights(fileName + \".h5\")\n",
        "            print('---------- Saving model to file, stats are: ----------')\n",
        "\n",
        "        AUCStr = (\"%0.4f\" % auroc )\n",
        "        AUPRStr = (\"%0.4f\" % auprc )\n",
        "        print(\"Win_AUC on validation set \" + AUCStr + \" AUPR: \" + AUPRStr)  \n",
        "        \n",
        "        print(\"Elapsed time: %s s\" % (time.time() - start_time))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUtSpqZveKTR",
        "outputId": "59b6da02-e328-47a2-eda1-8f5c2143ffd5"
      },
      "source": [
        "########################################################\n",
        "###DEFINING CALLBACKS AND STARTING MODEL FITTING...#####\n",
        "########################################################\n",
        "\n",
        "if training2do:\n",
        "    valSetCallBack = ModelTest(1, batch_size, np.nan, valUserIDs,\n",
        "                 data_path, samplesPerWin, channels2Use, convModelDims, siameseNet, modelID,useGenerator,winSize)\n",
        "    testSetCallBack = ModelTest(1, batch_size, np.nan, testUserIDs,\n",
        "                 data_path, samplesPerWin, channels2Use, convModelDims, siameseNet, modelID,useGenerator,winSize)\n",
        "    callbacks_list = [valSetCallBack,testSetCallBack]\n",
        "    \n",
        "    y_alls = []\n",
        "    y_alls_pos = []\n",
        "    y_alls_neg = []\n",
        "    for currUser in userIDs:\n",
        "        #y_alls.append(np.int16( pickle.load(open(data_path+ currUser + '/' + 'y.p'))))\n",
        "        aux = np.int16( pickle.load(open(data_path+ currUser + '/' + 'y.p', 'rb')))\n",
        "        y_alls_pos.append(np.nonzero(aux == 1)[0])\n",
        "        y_alls_neg.append(np.nonzero(aux == 0)[0])\n",
        "        \n",
        "  \n",
        "    #model fitting on data generators\n",
        "    hist = model.fit_generator(generator(data_path,batch_size,userIDs,channels2Use,samplesPerWin,convModelDims,siameseNet,y_alls_pos ,y_alls_neg,winSize),\n",
        "                               steps_per_epoch = (n_samples/batch_size), epochs = n_ep, use_multiprocessing = True,\n",
        "                               callbacks = callbacks_list, workers = workers, max_queue_size = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre-processing for validation user #: 0\n",
            "elapsed time for validation set pre-processing: 0.5692996978759766\n",
            "pre-processing for validation user #: 0\n",
            "elapsed time for validation set pre-processing: 0.824427604675293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.4967 AUPR: 0.1818\n",
            "Elapsed time: 15.122232675552368 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.5157 AUPR: 0.0902\n",
            "Elapsed time: 12.085102319717407 s\n",
            "Epoch 1/30\n",
            "102/102 [==============================] - 698s 7s/step - loss: 0.8719 - accuracy: 0.5091\n",
            "Win_AUC on validation set 0.4358 AUPR: 0.1359\n",
            "Elapsed time: 10.991071701049805 s\n",
            "Win_AUC on validation set 0.4988 AUPR: 0.0823\n",
            "Elapsed time: 11.004363059997559 s\n",
            "Epoch 2/30\n",
            "102/102 [==============================] - 611s 6s/step - loss: 0.7936 - accuracy: 0.5255\n",
            "Win_AUC on validation set 0.4983 AUPR: 0.1710\n",
            "Elapsed time: 11.104629755020142 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.5903 AUPR: 0.1205\n",
            "Elapsed time: 11.282002449035645 s\n",
            "Epoch 3/30\n",
            "102/102 [==============================] - 610s 6s/step - loss: 0.7760 - accuracy: 0.5522\n",
            "Win_AUC on validation set 0.4489 AUPR: 0.1451\n",
            "Elapsed time: 11.073890447616577 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.6917 AUPR: 0.1667\n",
            "Elapsed time: 11.335238695144653 s\n",
            "Epoch 4/30\n",
            "102/102 [==============================] - 613s 6s/step - loss: 0.7561 - accuracy: 0.5646\n",
            "Win_AUC on validation set 0.4362 AUPR: 0.1552\n",
            "Elapsed time: 11.142307758331299 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.6964 AUPR: 0.1753\n",
            "Elapsed time: 11.369319200515747 s\n",
            "Epoch 5/30\n",
            "102/102 [==============================] - 614s 6s/step - loss: 0.7314 - accuracy: 0.6280\n",
            "Win_AUC on validation set 0.4531 AUPR: 0.1744\n",
            "Elapsed time: 11.234677076339722 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.7528 AUPR: 0.2603\n",
            "Elapsed time: 11.418805122375488 s\n",
            "Epoch 6/30\n",
            "102/102 [==============================] - 603s 6s/step - loss: 0.6334 - accuracy: 0.7552\n",
            "Win_AUC on validation set 0.4521 AUPR: 0.1623\n",
            "Elapsed time: 11.093580722808838 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.8009 AUPR: 0.2993\n",
            "Elapsed time: 11.15047025680542 s\n",
            "Epoch 7/30\n",
            "102/102 [==============================] - 609s 6s/step - loss: 0.5981 - accuracy: 0.7606\n",
            "Win_AUC on validation set 0.4619 AUPR: 0.1558\n",
            "Elapsed time: 11.381060361862183 s\n",
            "---------- Saving model to file, stats are: ----------\n",
            "Win_AUC on validation set 0.8276 AUPR: 0.3445\n",
            "Elapsed time: 11.521746635437012 s\n",
            "Epoch 8/30\n",
            "102/102 [==============================] - 588s 6s/step - loss: 0.5692 - accuracy: 0.7882\n",
            "Win_AUC on validation set 0.4668 AUPR: 0.1448\n",
            "Elapsed time: 11.50714635848999 s\n",
            "Win_AUC on validation set 0.8153 AUPR: 0.2739\n",
            "Elapsed time: 11.37741208076477 s\n",
            "Epoch 9/30\n",
            "102/102 [==============================] - 561s 5s/step - loss: 0.5406 - accuracy: 0.7988\n",
            "Win_AUC on validation set 0.4804 AUPR: 0.1481\n",
            "Elapsed time: 11.750450849533081 s\n",
            "Win_AUC on validation set 0.8256 AUPR: 0.2664\n",
            "Elapsed time: 11.870133399963379 s\n",
            "Epoch 10/30\n",
            "102/102 [==============================] - 540s 5s/step - loss: 0.4813 - accuracy: 0.8222\n",
            "Win_AUC on validation set 0.4804 AUPR: 0.1446\n",
            "Elapsed time: 10.914566278457642 s\n",
            "Win_AUC on validation set 0.8167 AUPR: 0.2686\n",
            "Elapsed time: 10.960885047912598 s\n",
            "Epoch 11/30\n",
            "102/102 [==============================] - 545s 5s/step - loss: 0.5047 - accuracy: 0.8222\n",
            "Win_AUC on validation set 0.4904 AUPR: 0.1484\n",
            "Elapsed time: 11.425612211227417 s\n",
            "Win_AUC on validation set 0.8342 AUPR: 0.3304\n",
            "Elapsed time: 11.456790685653687 s\n",
            "Epoch 12/30\n",
            "102/102 [==============================] - 550s 5s/step - loss: 0.4898 - accuracy: 0.8243\n",
            "Win_AUC on validation set 0.5147 AUPR: 0.1655\n",
            "Elapsed time: 11.439698457717896 s\n",
            "Win_AUC on validation set 0.8284 AUPR: 0.2970\n",
            "Elapsed time: 11.424968719482422 s\n",
            "Epoch 13/30\n",
            "102/102 [==============================] - 551s 5s/step - loss: 0.4643 - accuracy: 0.8371\n",
            "Win_AUC on validation set 0.5171 AUPR: 0.1505\n",
            "Elapsed time: 11.487698078155518 s\n",
            "Win_AUC on validation set 0.8212 AUPR: 0.3176\n",
            "Elapsed time: 11.455381155014038 s\n",
            "Epoch 14/30\n",
            "102/102 [==============================] - 559s 5s/step - loss: 0.4516 - accuracy: 0.8371\n",
            "Win_AUC on validation set 0.5059 AUPR: 0.1504\n",
            "Elapsed time: 11.160701751708984 s\n",
            "Win_AUC on validation set 0.8111 AUPR: 0.3040\n",
            "Elapsed time: 11.173028469085693 s\n",
            "Epoch 15/30\n",
            "102/102 [==============================] - 551s 5s/step - loss: 0.4354 - accuracy: 0.8468\n",
            "Win_AUC on validation set 0.5292 AUPR: 0.1575\n",
            "Elapsed time: 11.012529134750366 s\n",
            "Win_AUC on validation set 0.7980 AUPR: 0.3045\n",
            "Elapsed time: 11.007777452468872 s\n",
            "Epoch 16/30\n",
            " 36/102 [=========>....................] - ETA: 5:54 - loss: 0.4024 - accuracy: 0.8637"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_gy0zOCirGw"
      },
      "source": [
        "# **Validating and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HLRmveLjR41"
      },
      "source": [
        "def get_data_windows(currMatFilePath,win_size_fs,downSampling,overlap,test):\n",
        "    val, _ = wfdb.rdsamp(currMatFilePath)\n",
        "    val = np.float32(val)\n",
        "    curr_user_n_dense_samples = val.shape[0]\n",
        "    val = val[::downSampling,:]\n",
        "    if not test:\n",
        "        with h5py.File(currMatFilePath+'-arousal.mat', 'r') as f:\n",
        "            arousalLabel = f['data']['arousals']\n",
        "            arousalLabel = arousalLabel[:]\n",
        "        arousalLabel = arousalLabel[::downSampling]\n",
        "    length = len(val)\n",
        "    startIdx = 0\n",
        "    finIdx = win_size_fs\n",
        "    deltaWindow = int(np.floor((1 - overlap)*win_size_fs))\n",
        "\n",
        "    winCount = 1\n",
        "    x_temp = []\n",
        "    y_temp = []\n",
        "    while finIdx <= length:\n",
        "        winVal = val[startIdx:finIdx,:]\n",
        "        x_temp.append(winVal)\n",
        "        currLabelVec = np.array(arousalLabel[startIdx:finIdx]);\n",
        "        currLabelVoting = [np.sum(currLabelVec == -1),np.sum(currLabelVec == 0),\n",
        "                           np.sum(currLabelVec == 1)]\n",
        "        idx = np.argmax(currLabelVoting);\n",
        "        idx = idx - 1;\n",
        "        \n",
        "        y_temp.append(    idx    )\n",
        "        winCount = winCount + 1\n",
        "        startIdx = startIdx + deltaWindow\n",
        "        finIdx = finIdx + deltaWindow\n",
        "        \n",
        "    x_temp = np.array(x_temp)\n",
        "    return x_temp, curr_user_n_dense_samples, y_temp\n",
        "\n",
        "\n",
        "\n",
        "def from_win_prediction_to_dense(model_output,win_upsample,overlap,len_y_dense):\n",
        "    '''\n",
        "        Uses the prediction of overlapping time windows (model_output) to compute dense prediction (that is @200Hz)\n",
        "    '''\n",
        "    dense_pred = np.zeros((len_y_dense,),dtype = np.float16)\n",
        "    count_pred = np.zeros((len_y_dense,),dtype = np.int16)\n",
        "    delta = int(np.floor((1.0-overlap)*win_upsample))\n",
        "    init = 0\n",
        "    fin = int(win_upsample)\n",
        "    for i in range(model_output.shape[0]):\n",
        "        dense_pred[init:fin] = dense_pred[init:fin] + model_output[i]\n",
        "        count_pred[init:fin] = count_pred[init:fin] + 1 \n",
        "        init = init + delta\n",
        "        fin = fin + delta\n",
        "    \n",
        "    for i ,count in enumerate(count_pred):\n",
        "        if count == 0:\n",
        "            count_pred[i] = 1\n",
        "            \n",
        "        \n",
        "    dense_pred = dense_pred / count_pred\n",
        "    \n",
        "\n",
        "    return dense_pred      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_input_data(x_temp,samplesPerWin,siameseNet,channels2Use,convModelDims,overlap,winSize):\n",
        "    flattened_channels = [item for sublist in channels2Use for item in sublist]\n",
        "    x_temp = x_temp[:,:,flattened_channels]\n",
        "    x_s = np.zeros((len(x_temp),samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "    if siameseNet >= 1:\n",
        "        x_s_prev = np.zeros((len(x_temp),samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "        if siameseNet >= 2:\n",
        "            x_s_next = np.zeros((len(x_temp),samplesPerWin,len(flattened_channels)),dtype = np.float32)\n",
        "        else:\n",
        "            x_s_next = []\n",
        "    else:\n",
        "        x_s_prev = []\n",
        "        x_s_next = []\n",
        "\n",
        "    for i in range(len(x_temp)):\n",
        "        x_s[i] = pre_processing(x_temp[i],flattened_channels,samplesPerWin,winSize)\n",
        "    \n",
        "    if siameseNet >= 1:\n",
        "        for i in range(len(x_temp)):\n",
        "            x_s_prev[i]  = pre_processing(x_temp[max(0,i-int(1./(1-overlap) ) )  ],flattened_channels,samplesPerWin,winSize, normFlag = False)\n",
        "    if siameseNet == 2:\n",
        "        for i in range(len(x_temp)):\n",
        "            x_s_next[i]  = pre_processing(x_temp[min(i+int(1./(1-overlap)),len(x_temp) - 1)],flattened_channels,samplesPerWin,winSize, normFlag = False)\n",
        "        \n",
        "    x_s = rearrange_channels(x_s,convModelDims,channels2Use)           \n",
        "    if siameseNet >= 1:\n",
        "        x_s_prev = rearrange_channels(x_s_prev,convModelDims,channels2Use)   \n",
        "    if siameseNet == 2:\n",
        "        x_s_next = rearrange_channels(x_s_next,convModelDims,channels2Use) \n",
        "    return x_s + x_s_prev + x_s_next\n",
        "\n",
        "def get_raw_data_path():\n",
        "    raw_data_path = '../data/training/'\n",
        "    return raw_data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34dU7GiiqUU"
      },
      "source": [
        "####################################################\n",
        "#Trying out the model on dense prediction settings#\n",
        "###################################################\n",
        "\n",
        "from submission_utils import get_data_windows, from_win_prediction_to_dense, prepare_input_data, get_raw_data_path\n",
        "from new_scoring_function import Challenge2018Score\n",
        "import h5py\n",
        "\n",
        "\n",
        "downSampling = 4\n",
        "overlap = 0.5\n",
        "repData = 5\n",
        "\n",
        "\n",
        "modelPath = data_path + modelID +  '_model'\n",
        "raw_data_path = get_raw_data_path() \n",
        "\n",
        "json_file = open(modelPath + '.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights(modelPath + '.h5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "scoreObj = Challenge2018Score()\n",
        "window_scoreObj = Challenge2018Score()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if testing2do:\n",
        "    for hhh, curr_user in enumerate(test_users):\n",
        "        sys.stdout.write(\"\\r\" + 'iteration: ' + str(hhh) + ' user: ' + curr_user)\n",
        "        sys.stdout.flush()\n",
        "        #print 'iteration: ' + str(hhh) + ' user: ' + curr_user\n",
        "        currMatFilePath = test_data_dir + curr_user + '/' + curr_user\n",
        "        y_window = [];\n",
        "        for _ in range(repData):\n",
        "            x_temp, curr_user_n_dense_samples, _  = get_data_windows(currMatFilePath,int(round(winSize*(200.0/float(downSampling)))),downSampling,overlap,True)\n",
        "            x_temp = prepare_input_data(x_temp,samplesPerWin,siameseNet,channels2Use,convModelDims,overlap,winSize)\n",
        "            if not len(y_window):\n",
        "                y_window = model.predict(x_temp)\n",
        "                y_window = y_window[:,1]\n",
        "            else:\n",
        "                temp_win = model.predict(x_temp)\n",
        "                y_window = y_window + temp_win[:,1]\n",
        "        y_window = y_window/repData\n",
        "        y_pred = np.round(from_win_prediction_to_dense(y_window, winSize*200,overlap,curr_user_n_dense_samples),3)\n",
        "        y_pred = y_pred[:,np.newaxis]\n",
        "        np.savetxt(\"../annotations/\" + curr_user + \".vec\", y_pred, fmt='%.3f')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if validation2do:\n",
        "    \n",
        "    scoreObj = Challenge2018Score()\n",
        "    window_scoreObj = Challenge2018Score()\n",
        "    \n",
        "    for hhh, curr_user in enumerate(valUserIDs):\n",
        "        sys.stdout.write(\"\\r\" + 'iteration: ' + str(hhh) + ' user: ' + curr_user)\n",
        "        sys.stdout.flush()\n",
        "        currMatFilePath = raw_data_path + curr_user + '/' + curr_user\n",
        "        y_window = [];\n",
        "        for _ in range(repData):\n",
        "            x_temp, curr_user_n_dense_samples, y_temp = get_data_windows(currMatFilePath,int(round(winSize*(200.0/float(downSampling)))),downSampling,overlap,False)\n",
        "            x_temp = prepare_input_data(x_temp,samplesPerWin,siameseNet,channels2Use,convModelDims,overlap,winSize)\n",
        "            if not len(y_window):\n",
        "                y_window = model.predict(x_temp)\n",
        "                y_window = y_window[:,1]\n",
        "            else:\n",
        "                temp_win = model.predict(x_temp)\n",
        "                y_window = y_window + temp_win[:,1]\n",
        "        y_window = y_window/repData\n",
        "                \n",
        "            \n",
        "    \n",
        "    \n",
        "        \n",
        "        \n",
        "        y_pred = np.round(from_win_prediction_to_dense(y_window, winSize*200,overlap,curr_user_n_dense_samples),3)\n",
        "        y_pred = y_pred[:,np.newaxis]\n",
        "        y_window = y_window[:,np.newaxis]\n",
        "        arousalFile = currMatFilePath + '-arousal.mat'\n",
        "        with h5py.File(arousalFile, 'r') as f:\n",
        "            arousalLabel = f['data']['arousals']\n",
        "            arousalLabel = arousalLabel[:]\n",
        "        window_scoreObj.score_record(np.asarray(y_temp)[np.asarray(y_temp) >= 0 ],y_window[np.asarray(y_temp) >= 0 ],curr_user)\n",
        "        scoreObj.score_record(arousalLabel, y_pred, curr_user)\n",
        "    print \n",
        "    print 'Dense level labelling: '\n",
        "    auroc = scoreObj.gross_auroc()\n",
        "    auprc = scoreObj.gross_auprc()\n",
        "    print 'AUROC: ' + str(auroc) \n",
        "    print 'AUPR: ' + str(auprc) \n",
        "    \n",
        "    print 'Window level labelling: '                 \n",
        "    auroc = window_scoreObj.gross_auroc()\n",
        "    auprc = window_scoreObj.gross_auprc()\n",
        "    print 'AUROC: ' + str(auroc) \n",
        "    print 'AUPR: ' + str(auprc) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}